# RAG 기반 LLM-as-a-Judge 데이터셋 구축 설정
# 참고: Self-Instruct, Evol-Instruct, RAFT, LLM-as-a-Judge, Prometheus

llm:
  generator:
    # 질문/답변 생성용 (저비용 모델 권장)
    # OpenAI-compatible API 규격
    base_url: "https://api.openai.com/v1"
    model: "gemini-3-pro-preview"
    api_key: "${API_KEY}"
    temperature: 0.7
    max_tokens: 2048
  
  judge:
    # SOTA 평가용 (고성능 모델 권장)
    # OpenAI-compatible API 규격
    base_url: "https://api.openai.com/v1"
    model: "gpt-5"
    api_key: "${API_KEY}"
    temperature: 0.0
    max_tokens: 1024

pipeline:
  # Self-Instruct 기반 설정 (Wang et al., 2023)
  seed_questions_per_chunk: 3
  question_diversity_threshold: 0.7
  
  # Evol-Instruct 기반 설정 (Xu et al., 2023)
  max_iterations: 3
  complexity_levels:
    - "basic"      # 단순 사실 질문
    - "intermediate"  # 관계 추론 질문
    - "advanced"   # 다단계 복합 질문
  
  # LLM-as-a-Judge 기반 설정 (Zheng et al., 2024)
  # Prometheus 루브릭 적용 (Kim et al., 2024)
  evaluation_criteria:
    - name: "groundedness"
      weight: 0.4
      description: "답변이 제공된 문서에만 기반하는가 (환각 방지)"
    - name: "accuracy"
      weight: 0.35
      description: "도메인 지식 측면에서 정확한가"
    - name: "completeness"
      weight: 0.25
      description: "질문에 충분한 정보를 제공하는가"
  
  pass_threshold: 4.5
  output_format: "jsonl"

rag:
  # RAFT 기반 설정 (Zhang et al., 2024)
  chunk_size: 512
  chunk_overlap: 50
  embedding_model: "sentence-transformers/all-MiniLM-L6-v2"
  vector_db: "chromadb"
  top_k: 5
  
  # PDF/이미지 처리 설정 (EasyOCR/PaddleOCR 기반 - 고성능/경량)
  use_ocr_fallback: true   # 텍스트 추출 실패 시 OCR 사용
  ocr_backend: "auto"      # auto(권장): EasyOCR > PaddleOCR > Tesseract 순
  ocr_lang: "korean"       # OCR 언어 (korean, eng 등)
  ocr_dpi: 200             # OCR 이미지 해상도 (엣지 환경 고려)
  ocr_use_gpu: false       # 엣지 환경 기본 CPU 사용
  extract_tables: true     # PDF 테이블 추출 여부

domain:
  name: "smartfarm"
  description: "스마트팜 도메인 (와사비, 토마토 등 작물 재배)"
  seed_questions:
    - "이 작물의 최적 재배 온도는?"
    - "양액 관리 시 주의사항은?"
    - "주요 병해충과 대처법은?"

# ---------------------------------------------------------------------------
# (Additive) Corpus / MT / MQM pipeline config for corpus_cli.py
# ---------------------------------------------------------------------------
corpus:
  # Official CGIAR datasets to export as EN corpus (JSON via `datasets`)
  cgiar_datasets:
    - "CGIAR/gardian-ai-ready-docs"
    - "CGIAR/cirad-ai-documents"
    - "CGIAR/ifpri-ai-documents"
  # Optional filter keywords for domain narrowing (leave empty to keep all)
  filter_keywords: []
  output_dir: "dataset/output"

translation:
  src_lang: "en"
  tgt_lang: "ko"
  # Optional glossary file (YAML/JSON mapping). Example:
  # glossary_path: "dataset/config/wasabi_glossary.yaml"
  glossary_path: ""

mqm:
  # Optional: multiple judges (e.g., GPT-5 + Gemini 3.0).
  # If not provided, corpus_cli falls back to `llm.judge`.
  judges:
    - name: "gpt5"
      base_url: "https://api.openai.com/v1"
      model: "gpt-5"
      api_key: "${API_KEY}"
      temperature: 0.0
      max_tokens: 1200
    - name: "gemini3"
      base_url: "https://api.openai.com/v1"
      model: "gemini-3-pro-preview"
      api_key: "${API_KEY}"
      temperature: 0.0
      max_tokens: 1200
  # Example:
  # judges:
  #   - name: "gpt5"
  #     base_url: "https://api.openai.com/v1"
  #     model: "gpt-5"
  #     api_key: "${API_KEY}"
  #     temperature: 0.0
  #     max_tokens: 1200
  #   - name: "gemini3"
  #     base_url: "https://api.openai.com/v1"
  #     model: "gemini-3.0"
  #     api_key: "${API_KEY}"
  #     temperature: 0.0
  #     max_tokens: 1200
